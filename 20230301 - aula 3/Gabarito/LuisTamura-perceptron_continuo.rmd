---
title: "Percepetron de Camada Simples para saídas contínuas"
author: "Prof. Leandro Augusto"
output: html_document
---

## Problema de regressão linear

Um problema de regressão pode também ser definido como sendo um problema de predição, no entanto, com a saída alvo (target) sendo um valor numérico, na maioria dos casos, contínuo. 

Exemplos típicos que usam técncias de regressão são em previsões de séries temporais. Ou seja, prever o valor futuro do número de clientes dado uma campanha de promoção, estimar o valor futuro de um indicador financeiro entre outros. 

Portanto, formalmente a estimação de um valor y é dada em função de um valor x. Ou seja, y=f(x). Na estatística os algoritmos classicos aplicados neste domínio de problemas é a regressão linear e logística.

Aqui será introduzido o conceito de Redes Neurais em problemas com valores de saída contínuo.

Para manter o foco no algoritmo, mais especificamente em sua parametrização, o problema a ser tratado será muito simples. Verificar se o algoritmo aprende a seguinte equação de uma reta Y<-2*(X)+3 com um total de 10 exemplares (N). A seguir estão a geração do dataset e a plotagem da reta a ser aprendida:

```{r}
N<-10
X<-seq(1:N)
Y<-2*(X)+3
plot(X,Y,type="o",xlim=c(0,10),ylim=c(0,25))
```

Aparentemente, esta atividade parece ser uma tarefa simples. No entanto, lembre-se que a rede neural tem que descobrir a equação da reta, sem ter essa informação, apenas os valores X e Y como um dataset.

Em se tratando do dataset, os valores serão normalizados pelo valor máximo (max) para ficarem no domínio de 0 a 1. E, em termos do X, após normalização, será adicionado uma coluna para o Bias (constante e com valor unitário):

```{r}
X<-X/max(X)
X<-t(t(X))
bias<-1
X<-cbind(X,bias)
print(X)
```

O valor de Y (saída desejada), também será normalizado pelo máximo da variável:

```{r}
Y<-Y/max(Y)
print(cbind(Y))
```


Agora que o dataset está pronto, é preciso estabelecer as parametrizações do algoritmo da Rede Neural que realizará o aprendizado da equação da reta.

Estes valores são de extrema importância para o sucesso da aplicação dos algoritmos de RNA. Portanto, os valores aqui definidos deverão depois serem melhor ajustados em forma de exercício prático:

```{r}
epoca_max<-10
eta<-0.01
W<-c(0,1)
```

Para que sejam feitas análises futuras em relação a efetividade do treinamento, vamos monitorar o erro do treinamento da rede a cada iteação e época. Por isso, criaremos variáveis auxiliares para este suporte:

```{r}
err_iter <- rep(0,N)
err_epoc <- rep(0,epoca_max)
```

Finalmente estabelece-se o algoritmo de aprendizado e o treinamento da rede acontece:

```{r}
for(epoca in 1:epoca_max){
  for(i in 1:N) {
    v<-sum(X[i,]*W)
    erro<-Y[i]-v
    delta<-(eta*erro*X[i,])
    W<-W+delta
    err_iter[i]<-0.5*(erro^2)
  }
  err_epoc[epoca]<-(sum(err_iter))
}
```

Ao final do treinamento, deseja-se analisar o erro a cada época de treinamento e o valor final dos pesos sinápticos.

```{r}
plot(err_epoc,type="line",xlab = "época", ylab = "erro quadrático médio",)
```


O que se deve esperar deste gráfico é um decaimento do erro em relação à época assintoticamente a zero. Aqui ainda poderia se pensar em definir como um segundo cirtério de parada do treinamento da rede um erro máximo permitido, pois o treinamento dificilmente conseguirá reduzir o erro à zero. 

Agora, deseja-se saber qual foi o peso sináptico da rede após o treinamento:

```{r}
print(W)
```

Estes valores de peso se equivalem respectivamente ao 2 e 3 da equação da reta (2*X + 3). No entanto, como houve normalização dos valores, a comparação não pode ser direta. Porém, uma maneira de analisar qualitativamente o treinamento da rede é fazendo uma plotagem sobreposta entre o gráfico X e Y apresentado no início do case e, agora, com o Y estimado (Y_estimado), usandos os valores de peso ajustados após o trenamento da rede.

Sendo assim, primeiramente encontrando o Y estimado:

```{r}
Y_estimado = X[,1]*W[1]+W[2] 
```

E, finalmente, o gráfico sobreposto:

```{r}
plot(X[,1],Y,type="n")#,ylim=c(0.5, 1.2))
points(X[,1],Y,pch=1,type="o",col="1")
points(X[,1],Y_estimado,pch=16,type="p",col="2")
legend(0.1,0.9,legend=c("Y_desejado","Y_estimado"),col=c(1,2), pch=c(1,16))
```

Note que, as escalas dos valores dos eixos não são os mesmos da primeira plotagem em razão da normalização dos dados. Contudo, o esperado de se encontrar aqui é os pontos vermelhos, Y estimado, se aproximarem do valor desejado, obviamente com um erro, que já foi calculado antes e é:

```{r}
print(paste("Erro final do treinamento",err_epoc[epoca_max]))
```

1) Aumente o número máximo de épocas (por exemplo, para 300) e apresente o gráfico de erro por época, como também o gráfico de sobreposição de saídas (desejada e estimada). Faça uma análise se há melhoras em relação aos valores anteriormente apresentados. Verifique o valor final de erro e veja se ele consegue chegar a 0 (zero).
```{r}
epoca_max<-300
eta<-0.01
W<-c(0,1)

err_iter <- rep(0,N)
err_epoc <- rep(0,epoca_max)


for(epoca in 1:epoca_max){
  for(i in 1:N) {
    v<-sum(X[i,]*W)
    erro<-Y[i]-v
    delta<-(eta*erro*X[i,])
    W<-W+delta
    err_iter[i]<-0.5*(erro^2)
  }
  err_epoc[epoca]<-(sum(err_iter))
}

print(W)
Y_estimado = X[,1]*W[1]+W[2]
```

Comentário do aluno: Por conta da quantidade maior de épocas temos uma curva com queda mais suave ao compararmos com o gráfico de apenas 10 épocas.
```{r}
plot(err_epoc,type="line",xlab = "época", ylab = "erro quadrático médio",)
```


Comentário do aluno: Devido a quantidade de épocas o algoritmo conseguiu treinar mais, portanto o erro final foi menor, além da predição do algoritmo ter sido consideravelmente melhor do que o seu concorrente de apenas 10 épocas.
```{r}
plot(X[,1],Y,type="n")
points(X[,1],Y,pch=1,type="o",col="1")
points(X[,1],Y_estimado,pch=16,type="p",col="2")
legend(0.1,0.9,legend=c("Y_desejado","Y_estimado"),col=c(1,2), pch=c(1,16))
```

Comentário do aluno: O erro está perto de 0, para alcançar 0 o programa necessita de mais épocas.
```{r}
print(paste("Erro final do treinamento",err_epoc[epoca_max]))
```

2) Agora, com este novo valor de época, altere o valor do eta para, por exemplo, 0.1. Análise o gráfico de erro e verifique se o ponto de diminuição abrupta (ponto em que faz o formato de "cotovelo" do gráfico) é anterior ou posterior ao do experimento anterior. Depois, refaça as tarefas pedidas no exercício 1 e compare os desempenhos. Por fim, conclua em que implica a alteração deste parâmetro e faça uma interpretação do que se espera acontecer se o valor deste parâmetro por aumentado (tendendo a 1).

```{r}
epoca_max<-300
eta<-0.1
W<-c(0,1)

err_iter <- rep(0,N)
err_epoc <- rep(0,epoca_max)


for(epoca in 1:epoca_max){
  for(i in 1:N) {
    v<-sum(X[i,]*W)
    erro<-Y[i]-v
    delta<-(eta*erro*X[i,])
    W<-W+delta
    err_iter[i]<-0.5*(erro^2)
  }
  err_epoc[epoca]<-(sum(err_iter))
}

print(W)
Y_estimado = X[,1]*W[1]+W[2]
```


Comentário do aluno: Devido a porcentagem de aprendizagem ser maior (eta) que a do exercício anterior o ponto de diminuição abrupta é anterior, quanto maior o eta mais rápido este "cotovelo" aparece.
```{r}
plot(err_epoc,type="line",xlab = "época", ylab = "erro quadrático médio",)
```

Comentário do aluno: Devido ao eta maior o Y_estimado conseguiu ser equivalente ao Y_desejado, ao compararmos com o gráfico do exercício anterior, vemos que o eta influencia na taxa de aprendizagem do algoritmo, pois mesmo com uma quantidade de épocas semelhantes, um algoritmo se saiu melhor do que o outro.
```{r}
plot(X[,1],Y,type="n")
points(X[,1],Y,pch=1,type="o",col="1")
points(X[,1],Y_estimado,pch=16,type="p",col="2")
legend(0.1,0.9,legend=c("Y_desejado","Y_estimado"),col=c(1,2), pch=c(1,16))
```
Comentário do aluno: Se mantivermos a quantidade de épocas em 300, porém aumentarmos mais o eta (até 1) o cotovelo do gráfico acontece anteriormente ao compararmos com os gráficos com eta menor; é possível que o algoritmo tenha um overfitting para o dataset atual, por conta do seu eta.


Comentário do aluno: Algo interessante, foi que o erro final do treinamento foi estrondosamente maior ao saior do eta de 0.01 para 0.1
```{r}
print(paste("Erro final do treinamento",err_epoc[epoca_max]))
```


3) Por fim, refaça os mesmos ensaios e análises da questão 2, agora alterando o valor inicial dos pesos sinápticos para W<-c(0.1,1).

```{r}
epoca_max<-300
eta<-0.1
W<-c(0.1,1)

err_iter <- rep(0,N)
err_epoc <- rep(0,epoca_max)


for(epoca in 1:epoca_max){
  for(i in 1:N) {
    v<-sum(X[i,]*W)
    erro<-Y[i]-v
    delta<-(eta*erro*X[i,])
    W<-W+delta
    err_iter[i]<-0.5*(erro^2)
  }
  err_epoc[epoca]<-(sum(err_iter))
}

print(W)
Y_estimado = X[,1]*W[1]+W[2]
```

Comentário do aluno: O cotovelo do gráfico aconteceu posteriormente ao compararmos com o feito no exercício anterior, porém os pesos contidos no vetor W são semelhantes aos do algoritmo sem a alteração do peso.
```{r}
plot(err_epoc,type="line",xlab = "época", ylab = "erro quadrático médio",)
```


Comentário do aluno: O algoritmo continua sendo capaz de acertar os valores do Y_estimado com os de Y_desejado mesmo com a alteração do peso sináptico.
```{r}
plot(X[,1],Y,type="n")
points(X[,1],Y,pch=1,type="o",col="1")
points(X[,1],Y_estimado,pch=16,type="p",col="2")
legend(0.1,0.9,legend=c("Y_desejado","Y_estimado"),col=c(1,2), pch=c(1,16))
```

Comentário do aluno: O peso sináptico ou a força da conexão acaba influênciando no erro final, mesmo o valor sendo alto, é menor ao valor obtido do exercício 2, além disso com o aumento do peso sináptico (até 1) a curva do gráfico torna-se mais rápida quase como uma queda livre.
```{r}
print(paste("Erro final do treinamento",err_epoc[epoca_max]))
```