---
title: "Trabalho RNA Continuo"
author: "AfonsoBrandao"
date: "2023-03-07"
output: pdf_document
---
<style>
  .exerc-legend{
    background:#e6f5ff;border-top: 2px solid #006bb3;
    }
  .exerc{
    background:#006bb3;color:#fff;padding:5px;
  }
  .enunciado{
    background:#e6f5ff;margin:-10px 0px 0px 15px;padding:10px 0px 20px 0px;border-bottom: 2px solid #006bb3
  }
</style>
  
## Primeira questão

1) Aumente o número máximo de épocas (por exemplo, para 300) e apresente o gráfico de erro por época, como também o gráfico de sobreposição de saídas (desejada e estimada). Faça uma análise se há melhoras em relação aos valores anteriormente apresentados. Verifique o valor final de erro e veja se ele consegue chegar a 0 (zero).


- Regressão modelo
```{r}
# Definindo o tamanho do vetor
N<-10

# Criando o vetor X com valores de 1 a N
X<-seq(1,N)

# Criando o vetor Y com a função 2X + 3
Y<-2*X+3

# Plotando os valores de X e Y em um gráfico de pontos
plot(X,Y,type="o",xlim=c(0, 10),ylim=c(0,25))
```
- Ajustes
```{r}
# Normalizando os vetores entre 0 e 1 e transpondo
X<-X/max(X)
Y<-Y/max(Y)
X<-t(t(X))

# Adicionando um vetor de bias à matriz X
bias<-1
X<-cbind(X,bias)

# Imprimindo as matrizes X e Y
print(X)
print(cbind(Y))
```

- Definindo variáveis e treinando
```{r}
# Definindo o número máximo de épocas e a taxa de aprendizado
epoca_max<-300
eta<-0.01

# Inicializando o vetor de pesos W
W<-c(0,1)

# Inicializando vetores para armazenar os erros das iterações e das épocas
err_iter<-rep(0,N)
err_epoc<-rep(0,epoca_max)

# Iniciando o loop de treinamento
for (epoca in 1:epoca_max) {
  for (i in 1:N) {
    # Calculando a saída estimada para a entrada atual
    v <- sum(X[i,]*W)
    # Calculando o erro entre a saída desejada e a saída estimada
    erro <- Y[i] - v
    # Calculando o delta para atualizar os pesos
    delta <- eta*erro*X[i,]
    # Atualizando os pesos
    W <- W + delta
    # Armazenando o erro da iteração atual
    err_iter[i] <- 0.5*(erro^2)
  }
  # Armazenando o erro da época atual
  err_epoc[epoca] <- sum(err_iter)
}

# Plotando o erro quadrático médio ao longo das épocas
plot(err_epoc, type = "line", xlab = "época", ylab = "erro quadrático médio")
```
- Pesos finais
```{r}
print(W)
```

- Calculando a saída estimada para todas as entradas e Plotando as saídas desejadas e estimadas em um gráfico
```{r}
Y_estimado<-X[,1]*W[1]+W[2]

plot(X[,1],Y,type ="n")
points(X[,1],Y,pch=1,type="o",col="1")
points(X[,1],Y_estimado,pch=16,type="p",col="2")
legend(0.1,0.9,legend=c("Y_desejado","Y_estimado"),col=c(1,2),pch=c(1,16))
```
```{r}
print(paste("Erro final do treinamento", err_epoc[epoca_max]))
```
### Resposta
Considerando o contexto abordado, foi possível observar um resultado significativo ao aumentar o número máximo de épocas para 300 no treinamento da rede neural. A partir do gráfico de erro por época, verificou-se que o erro inicialmente elevado apresentou uma rápida redução nas primeiras iterações, mas, posteriormente, a redução foi gradual, culminando em uma estabilização em torno de 0,103. Esse comportamento sugere que a rede neural está sendo capaz de aprender a relação entre as entradas e as saídas, porém ainda é possível considerar melhorias no processo.

Com relação ao gráfico de sobreposição de saídas, foi possível constatar que as saídas estimadas apresentaram uma boa aproximação em relação às saídas desejadas, com discrepâncias mínimas entre elas. Esse resultado evidencia que a rede neural está realizando uma aproximação satisfatória da função que gera as saídas, contribuindo para aprimoramentos futuros no processo de treinamento.

## Segunda Questão
2) Agora, com este novo valor de época, altere o valor do eta para, por exemplo, 0.1. Análise o gráfico de erro e verifique se o ponto de diminuição abrupta (ponto em que faz o formato de "cotovelo" do gráfico) é anterior ou posterior ao do experimento anterior. Depois, refaça as tarefas pedidas no exercício 1 e compare os desempenhos. Por fim, conclua em que implica a alteração deste parâmetro e faça uma interpretação do que se espera acontecer se o valor deste parâmetro por aumentado (tendendo a 1).

- Segue código condensado mudando a eta<- 0.01
```{r}
N<-10
X<-seq(1,N)
Y<-2*X+3
plot(X,Y,type="o",xlim=c(0, 10),ylim=c(0,25))
```

```{r}
X<-X/max(X)
Y<-Y/max(Y)
X<-t(t(X))
bias<-1
X<-cbind(X,bias)
print(X)
print(cbind(Y))
```
```{r}
epoca_max<-300
eta<-0.1
W<-c(0,1)
err_iter<-rep(0,N)
err_epoc<-rep(0,epoca_max)

for (epoca in 1:epoca_max) {
  for (i in 1:N) {
    v <- sum(X[i,]*W)
    erro <- Y[i] - v
    delta <- eta*erro*X[i,]
    W <- W + delta
    err_iter[i] <- 0.5*(erro^2)
  }
  err_epoc[epoca] <- sum(err_iter)
}
plot(err_epoc, type = "line", xlab = "época", ylab = "erro quadrático médio")
```

```{r}
print(W)

Y_estimado<-X[,1]*W[1]+W[2]

plot(X[,1],Y,type ="n")
points(X[,1],Y,pch=1,type="o",col="1")
points(X[,1],Y_estimado,pch=16,type="p",col="2")
legend(0.1,0.9,legend=c("Y_desejado","Y_estimado"),col=c(1,2),pch=c(1,16))

print(paste("Erro final do treinamento", err_epoc[epoca_max]))
```

### Resposta

Com o valor de eta alterado para 0.1 e mantendo o número máximo de épocas em 300, foi possível observar no gráfico de erro por época que o ponto de diminuição abrupta ocorreu em torno da época 20, ou seja, posterior ao experimento anterior com eta de 0.01.

Ao refazer as tarefas pedidas no exercício 1, verificou-se que a rede neural apresentou um desempenho melhor em relação ao experimento anterior, tendo alcançado um valor final de erro próximo de zero e uma aproximação mais precisa entre as saídas desejadas e estimadas.

A alteração do valor do parâmetro eta afeta diretamente a taxa de aprendizado da rede neural. Quando esse valor é aumentado, a rede pode aprender mais rápido, mas também pode ocorrer o risco de que ela fique presa em mínimos locais do erro e, portanto, não consiga aprender de forma mais precisa. Isso pode afetar o desempenho geral da rede neural.